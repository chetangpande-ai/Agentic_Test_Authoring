{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d878f143",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Chetan Pande\\AI-ML-2025\\agentic-ai-2.0\\Agentic_Test_Authoring\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dc71979",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c98cde78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(prompt.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66613453",
   "metadata": {},
   "source": [
    "[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, \n",
    "template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "229e7d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "model=ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d94e4401",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=model.invoke(\"hi, how r u?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2febff62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there!\n",
      "\n",
      "As an AI, I don't have feelings or a physical state, but I'm functioning perfectly and ready to assist you.\n",
      "\n",
      "How are you doing today, and what can I help you with?\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "898e995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "[\n",
    "    (\"system\",\"You are an expert AI Engineer. Provide me answer based on the question\"),\n",
    "    (\"user\",\"{input}\")\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bab6c8e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGoogleGenerativeAI(model='models/gemini-2.5-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001E375C6D7F0>, default_metadata=(), model_kwargs={})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain=prompt|model\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "598af753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith is a **developer platform for building, debugging, testing, and monitoring Large Language Model (LLM) applications**. It's developed by the LangChain team and is designed to bring traditional software development best practices (like tracing, logging, and evaluation) to the often-opaque world of LLM-powered systems.\n",
      "\n",
      "Think of it as the \"observability and MLOps platform\" specifically tailored for LLM applications.\n",
      "\n",
      "Here's a breakdown of what LangSmith offers and why it's crucial:\n",
      "\n",
      "### The Problem LangSmith Solves\n",
      "\n",
      "Developing LLM applications is challenging due to several factors:\n",
      "\n",
      "1.  **Non-determinism:** LLMs don't always produce the same output for the same input, making debugging difficult.\n",
      "2.  **Opacity (\"Black Box\"):** It's hard to understand *why* an LLM or an agent made a particular decision or produced a certain output.\n",
      "3.  **Complex Chains:** Modern LLM apps often involve multiple LLM calls, tool uses, retrievals, and custom logic, making the flow hard to follow.\n",
      "4.  **Evaluation Difficulty:** How do you objectively measure the performance, correctness, or \"goodness\" of an LLM's output?\n",
      "5.  **Production Monitoring:** Once deployed, how do you track performance, costs, errors, and user feedback?\n",
      "6.  **Iteration Speed:** Experimenting with different prompts, models, or chain architectures can be slow and hard to compare.\n",
      "\n",
      "### Key Features of LangSmith\n",
      "\n",
      "LangSmith addresses these challenges with a suite of powerful features:\n",
      "\n",
      "1.  **Tracing & Debugging:**\n",
      "    *   **Visualize Call Chains:** It allows you to see the entire execution flow of your LLM application, including every LLM call, tool use, retrieval step, and intermediate thought process of an agent.\n",
      "    *   **Detailed Step Information:** For each step, you can view inputs, outputs, timestamps, latency, token usage, costs, and any errors.\n",
      "    *   **\"Ghost in the Machine\":** This helps you understand *exactly* what happened during a run, making it much easier to pinpoint errors, prompt issues, or unexpected agent behavior.\n",
      "    *   **Reproducibility:** You can easily re-run a specific trace to experiment with changes.\n",
      "\n",
      "2.  **Testing & Evaluation:**\n",
      "    *   **Dataset Creation:** You can create and manage datasets of inputs and desired outputs (ground truth) or use traces from production as test cases.\n",
      "    *   **Automated Evaluators:** Run your application against these datasets and use built-in or custom evaluators (e.g., for correctness, toxicity, hallucination, latency, cost) to get objective scores.\n",
      "    *   **Human-in-the-Loop Evaluation:** Integrate human feedback and labeling to refine your evaluations.\n",
      "    *   **Comparison Views:** Easily compare the performance of different prompts, models, or chain architectures side-by-side on the same test set. This is crucial for A/B testing and iteration.\n",
      "\n",
      "3.  **Monitoring & Observability:**\n",
      "    *   **Production Monitoring:** Track key metrics like latency, token usage, cost, error rates, and user feedback from your deployed applications.\n",
      "    *   **Alerting:** Set up alerts for anomalies or performance degradation.\n",
      "    *   **Feedback Loops:** Capture user feedback directly from your application to identify areas for improvement and create new test cases.\n",
      "\n",
      "4.  **Prompt Management & Versioning:**\n",
      "    *   While not a dedicated prompt management system, LangSmith implicitly helps by allowing you to track and compare different prompt versions through your traces and evaluation runs.\n",
      "\n",
      "5.  **Collaboration:**\n",
      "    *   Share traces, datasets, and evaluation results with your team, fostering better collaboration and knowledge sharing.\n",
      "\n",
      "### How it Integrates\n",
      "\n",
      "LangSmith is primarily designed to integrate seamlessly with the LangChain framework. By simply setting a few environment variables (`LANGCHAIN_TRACING_V2=true`, `LANGCHAIN_API_KEY`, `LANGCHAIN_PROJECT`), your LangChain applications will automatically send their traces to LangSmith.\n",
      "\n",
      "However, it's also possible to use LangSmith with other LLM frameworks or even custom Python code by explicitly sending runs to the LangSmith API.\n",
      "\n",
      "### Who Uses It?\n",
      "\n",
      "*   **LLM Developers:** For debugging complex agents, chains, and prompt engineering.\n",
      "*   **ML Engineers:** For setting up robust evaluation pipelines and monitoring production systems.\n",
      "*   **Data Scientists:** For understanding model behavior and iterating on application logic.\n",
      "*   **Teams building any kind of LLM-powered application:** From chatbots and summarization tools to complex autonomous agents.\n",
      "\n",
      "### In essence:\n",
      "\n",
      "LangSmith is an indispensable tool for serious LLM application development. It transforms the \"black box\" nature of LLMs into an observable and manageable system, enabling faster iteration, higher quality applications, and greater confidence in deploying LLM-powered solutions.\n"
     ]
    }
   ],
   "source": [
    "response=chain.invoke({\"input\":\"Can you tell me something about Langsmith\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99638e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ishan Pande\\AppData\\Local\\Temp\\ipykernel_11928\\4088018816.py:2: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain\n",
    "chain = LLMChain(\n",
    "    llm=model,\n",
    "    prompt=prompt  # use the Hub-pulled prompt here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15523d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=chain.invoke(\n",
    "    {\n",
    "   \n",
    "    \"input\": \"Hello, who r u?\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbf00d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Hello, who r u?',\n",
       " 'text': 'Hello! I am a large language model, trained by Google.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30e7e9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I am a large language model, trained by Google.\n"
     ]
    }
   ],
   "source": [
    "print(result['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
